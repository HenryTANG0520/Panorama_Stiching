{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import gc\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在提取帧...\n",
      "总帧数: 6746\n",
      "从第 3 帧开始，间隔 60 帧\n",
      "预计处理帧数: 112\n",
      "提取帧进度: 113/112\n",
      "帧提取完成\n"
     ]
    }
   ],
   "source": [
    "def create_folders():\n",
    "    \"\"\"创建必要的文件夹结构\"\"\"\n",
    "    folders = [\n",
    "        'temp_frames',\n",
    "        'output'\n",
    "    ]\n",
    "    \n",
    "    for folder in folders:\n",
    "        Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_frames(video_path, start_frame, frame_interval):\n",
    "    \"\"\"\n",
    "    从指定帧开始提取帧\n",
    "    Args:\n",
    "        video_path: 视频路径\n",
    "        start_frame: 开始帧的索引\n",
    "        frame_interval: 帧间隔\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"无法打开视频文件: {video_path}\")\n",
    "    \n",
    "    try:\n",
    "        frame_count = 0\n",
    "        saved_count = 0\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        print(f\"总帧数: {total_frames}\")\n",
    "        print(f\"从第 {start_frame} 帧开始，间隔 {frame_interval} 帧\")\n",
    "        print(f\"预计处理帧数: {(total_frames - start_frame) // frame_interval}\")\n",
    "        \n",
    "        # 跳到指定的开始帧\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            if frame_count % frame_interval == 0:\n",
    "                cv2.imwrite(f'temp_frames/frame_{saved_count}.jpg', frame)\n",
    "                print(f\"\\r提取帧进度: {saved_count + 1}/{(total_frames - start_frame) // frame_interval}\", end=\"\", flush=True)\n",
    "                saved_count += 1\n",
    "                \n",
    "            frame_count += 1\n",
    "            del frame\n",
    "            gc.collect()\n",
    "        \n",
    "        print(\"\\n帧提取完成\")\n",
    "        return saved_count\n",
    "        \n",
    "    finally:\n",
    "        cap.release()\n",
    "        gc.collect()\n",
    "        \n",
    "def cleanup():\n",
    "    \"\"\"清理临时文件\"\"\"\n",
    "    try:\n",
    "        shutil.rmtree('temp_frames', ignore_errors=True)\n",
    "    except Exception as e:\n",
    "        print(f\"清理临时文件时出错: {str(e)}\")       \n",
    "\n",
    "# 创建必要的文件夹\n",
    "create_folders()\n",
    "Path('checkpoints').mkdir(exist_ok=True)\n",
    "\n",
    "video_path = \"video/4.mp4\"\n",
    "start_frame = 3\n",
    "frame_interval = 60\n",
    "# 提取帧\n",
    "print(\"正在提取帧...\")\n",
    "total_frames = extract_frames(video_path, start_frame, frame_interval)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=4, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=4, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=4, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 保持特征图的空间信息\n",
    "        f1 = self.relu(self.bn1(self.conv1(x)))\n",
    "        p1 = self.pool(f1)\n",
    "        \n",
    "        f2 = self.relu(self.bn2(self.conv2(p1)))\n",
    "        p2 = self.pool(f2)\n",
    "        \n",
    "        f3 = self.relu(self.bn3(self.conv3(p2)))\n",
    "        \n",
    "        return [f1, f2, f3]\n",
    "\n",
    "class CorrelationLayer(nn.Module):\n",
    "    def __init__(self, chunk_size=1000):\n",
    "        super().__init__()\n",
    "        self.chunk_size = chunk_size\n",
    "\n",
    "    def forward(self, feat1, feat2):\n",
    "        b, c, h, w = feat1.size()\n",
    "\n",
    "        # 展平特征图\n",
    "        feat1_flat = feat1.view(b, c, -1)\n",
    "        feat2_flat = feat2.view(b, c, -1)\n",
    "\n",
    "        # 计算总的位置数量\n",
    "        total_positions = h * w\n",
    "\n",
    "        # 初始化相关性张量\n",
    "        correlation = torch.zeros(b, total_positions, h, w, device=feat1.device)\n",
    "\n",
    "        # 分块计算相关性\n",
    "        for i in range(0, total_positions, self.chunk_size):\n",
    "            end_idx = min(i + self.chunk_size, total_positions)\n",
    "            # 只计算当前块的相关性\n",
    "            curr_correlation = torch.bmm(\n",
    "                feat1_flat[:, :, i:end_idx].permute(0, 2, 1),\n",
    "                feat2_flat\n",
    "            )\n",
    "            correlation[:, i:end_idx] = curr_correlation.view(b, end_idx-i, h, w)\n",
    "\n",
    "            # 主动清理内存\n",
    "            del curr_correlation\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return F.softmax(correlation, dim=1)\n",
    "\n",
    "class StitchingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = FeatureExtractor()\n",
    "        self.correlation = CorrelationLayer()\n",
    "        \n",
    "        # 自适应特征融合\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # 融合权重预测\n",
    "        self.blend_weights = nn.Conv2d(64, 1, 1)\n",
    "        \n",
    "    def forward(self, img1, img2):\n",
    "        # 提取多尺度特征\n",
    "        feats1 = self.feature_extractor(img1)\n",
    "        feats2 = self.feature_extractor(img2)\n",
    "        \n",
    "        # 计算特征相关性\n",
    "        correlations = []\n",
    "        for f1, f2 in zip(feats1, feats2):\n",
    "            corr = self.correlation(f1, f2)\n",
    "            correlations.append(corr)\n",
    "        \n",
    "        # 特征融合\n",
    "        fusion_feats = torch.cat([feats1[-1], feats2[-1]], dim=1)\n",
    "        fused = self.fusion(fusion_feats)\n",
    "        \n",
    "        # 预测融合权重\n",
    "        weights = torch.sigmoid(self.blend_weights(fused))\n",
    "        \n",
    "        # 上采样权重到原始图像大小\n",
    "        weights = F.interpolate(weights, size=img1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # 生成最终结果\n",
    "        result = weights * img1 + (1 - weights) * img2\n",
    "        \n",
    "        return result, weights, correlations\n",
    "\n",
    "class StitchingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, result, img1, img2, weights, correlations):\n",
    "        # 重建损失\n",
    "        reconstruct_loss = self.l1_loss(result, img2)\n",
    "        \n",
    "        # 相关性一致性损失\n",
    "        correlation_loss = sum(self.mse_loss(corr, torch.ones_like(corr)/corr.shape[1]) \n",
    "                             for corr in correlations)\n",
    "        \n",
    "        # 平滑度损失\n",
    "        smoothness_loss = self.l1_loss(weights[:,:,1:,:], weights[:,:,:-1,:]) + \\\n",
    "                         self.l1_loss(weights[:,:,:,1:], weights[:,:,:,:-1])\n",
    "        \n",
    "        total_loss = reconstruct_loss + 0.1 * correlation_loss + 0.01 * smoothness_loss\n",
    "        return total_loss\n",
    "\n",
    "def train_step(model, optimizer, img1, img2):\n",
    "    \"\"\"单步训练函数\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 前向传播\n",
    "    result, weights, correlations = model(img1, img2)\n",
    "    \n",
    "    # 计算损失\n",
    "    loss = StitchingLoss()(result, img1, img2, weights, correlations)\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FramePairsDataset(Dataset):\n",
    "    def __init__(self, frames_dir, size=(128, 128)):\n",
    "        self.frames_dir = Path(frames_dir)\n",
    "        self.frame_pairs = self._get_frame_pairs()\n",
    "        self.size = size\n",
    "        \n",
    "        # 使用PIL Image进行转换\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x[:3] if x.size(0) > 3 else x),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            frame1_path, frame2_path = self.frame_pairs[idx]\n",
    "            \n",
    "            # 读取并预处理图像\n",
    "            frame1 = self._safe_read_image(frame1_path)\n",
    "            frame2 = self._safe_read_image(frame2_path)\n",
    "            \n",
    "            # 转换为张量\n",
    "            frame1_tensor = self.transform(frame1)\n",
    "            frame2_tensor = self.transform(frame2)\n",
    "            \n",
    "            # 打印张量形状用于调试\n",
    "            print(f\"Tensor shapes - frame1: {frame1_tensor.shape}, frame2: {frame2_tensor.shape}\")\n",
    "            \n",
    "            return frame1_tensor, frame2_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {idx}: {str(e)}\")\n",
    "            # 返回全零张量\n",
    "            return torch.zeros(3, *self.size), torch.zeros(3, *self.size)\n",
    "        \n",
    "    def _get_frame_pairs(self):\n",
    "        \"\"\"获取相邻帧对，确保文件存在且可读\"\"\"\n",
    "        frames = []\n",
    "        for frame in sorted(list(self.frames_dir.glob('frame_*.jpg'))):\n",
    "            if frame.exists() and frame.stat().st_size > 0:\n",
    "                frames.append(frame)\n",
    "                \n",
    "        if not frames:\n",
    "            raise RuntimeError(f\"No valid frames found in {self.frames_dir}\")\n",
    "            \n",
    "        return [(frames[i], frames[i+1]) for i in range(len(frames)-1)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frame_pairs)\n",
    "    \n",
    "    def _safe_read_image(self, path):\n",
    "        \"\"\"安全地读取和处理图像，确保输出为3通道RGB图像\"\"\"\n",
    "        try:\n",
    "            # 使用IMREAD_COLOR确保读取为3通道\n",
    "            img = cv2.imread(str(path), cv2.IMREAD_COLOR)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Failed to read image: {path}\")\n",
    "            \n",
    "            # 调整图像大小以确保一致性\n",
    "            img = cv2.resize(img, self.size, interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            # 确保是3通道RGB图像\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # 确保图像为uint8类型\n",
    "            img = img.astype(np.uint8)\n",
    "            \n",
    "            # 打印图像形状和类型信息（用于调试）\n",
    "            # print(f\"Image shape: {img.shape}, dtype: {img.dtype}\")\n",
    "            \n",
    "            return img\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading image {path}: {str(e)}\")\n",
    "            # 返回一个空白图像而不是失败\n",
    "            return np.zeros((*self.size, 3), dtype=np.uint8)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            frame1_path, frame2_path = self.frame_pairs[idx]\n",
    "            \n",
    "            # 安全读取图像\n",
    "            frame1 = self._safe_read_image(frame1_path)\n",
    "            frame2 = self._safe_read_image(frame2_path)\n",
    "            \n",
    "            # 应用变换\n",
    "            if self.transform:\n",
    "                try:\n",
    "                    frame1 = self.transform(frame1)\n",
    "                    frame2 = self.transform(frame2)\n",
    "                except Exception as e:\n",
    "                    print(f\"Transform error for index {idx}: {str(e)}\")\n",
    "                    # 返回零张量而不是失败\n",
    "                    frame1 = torch.zeros((3, *self.size))\n",
    "                    frame2 = torch.zeros((3, *self.size))\n",
    "            \n",
    "            return frame1, frame2\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing item {idx}: {str(e)}\")\n",
    "            # 返回零张量而不是失败\n",
    "            return torch.zeros((3, *self.size)), torch.zeros((3, *self.size))\n",
    "\n",
    "def train_model(model, train_loader, num_epochs, device):\n",
    "    \"\"\"训练模型\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = StitchingLoss()\n",
    "\n",
    "    # 用于提前停止的变量\n",
    "    best_loss = float('inf')\n",
    "    patience = 5\n",
    "    patience_counter = 0\n",
    "    avg_loss = float('inf')  # 初始化avg_loss\n",
    "\n",
    "    # 创建进度条\n",
    "    epoch_pbar = tqdm(total=num_epochs, desc=\"Training Progress\")\n",
    "\n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            batch_count = 0\n",
    "\n",
    "            # 添加梯度累积\n",
    "            accumulation_steps = 4  # 累积4次更新一次\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 创建每个epoch的batch进度条\n",
    "            batch_pbar = tqdm(total=len(train_loader),\n",
    "                            desc=f\"Epoch {epoch+1}/{num_epochs}\",\n",
    "                            leave=False)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            for batch_idx, (img1, img2) in enumerate(train_loader):\n",
    "                try:\n",
    "                    img1, img2 = img1.to(device), img2.to(device)\n",
    "\n",
    "                    # 清除GPU缓存\n",
    "                    if batch_idx % 5 == 0:\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "                    # 前向传播和损失计算\n",
    "                    result, weights, correlations = model(img1, img2)\n",
    "                    loss = criterion(result, img1, img2, weights, correlations)\n",
    "                    loss = loss / accumulation_steps  # 缩放loss\n",
    "                    loss.backward()\n",
    "\n",
    "                    # 累积梯度\n",
    "                    if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "                    batch_count += 1\n",
    "\n",
    "                    # 更新batch进度条\n",
    "                    batch_pbar.update(1)\n",
    "                    batch_pbar.set_postfix({\n",
    "                        'loss': f'{loss.item():.4f}',\n",
    "                        'avg_loss': f'{total_loss/batch_count:.4f}'\n",
    "                    })\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\\\nError in batch {batch_idx}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            batch_pbar.close()\n",
    "\n",
    "            # 计算平均损失\n",
    "            avg_loss = total_loss / batch_count if batch_count > 0 else float('inf')\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            # 更新epoch进度条\n",
    "            epoch_pbar.update(1)\n",
    "            epoch_pbar.set_postfix({\n",
    "                'avg_loss': f'{avg_loss:.4f}',\n",
    "                'time': f'{epoch_time:.1f}s'\n",
    "            })\n",
    "\n",
    "            # 提前停止检查\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                patience_counter = 0\n",
    "                # 保存最佳模型\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': avg_loss,\n",
    "                }, 'checkpoints/best_model.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # 定期保存检查点\n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': avg_loss,\n",
    "                }, f'checkpoints/model_epoch_{epoch+1}.pt')\n",
    "\n",
    "            # 如果连续多个epoch没有改善，提前停止\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\\\nEarly stopping after {epoch+1} epochs\")\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\\\nTraining interrupted by user\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nTraining error: {str(e)}\")\n",
    "    finally:\n",
    "        epoch_pbar.close()\n",
    "        # 确保最后一个模型状态被保存\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, 'checkpoints/final_model.pt')\n",
    "\n",
    "\n",
    "def stitch_with_model(model, img1, img2, device, target_size=(256, 256)):\n",
    "    \"\"\"使用训练好的模型进行图像拼接，使用更小的目标尺寸\"\"\"\n",
    "    model.eval()\n",
    "    try:\n",
    "        with torch.cuda.amp.autocast():  # 使用混合精度\n",
    "            with torch.no_grad():\n",
    "                # 转换为较小的尺寸\n",
    "                if isinstance(img1, np.ndarray):\n",
    "                    img1 = torch.from_numpy(img1).permute(2, 0, 1).float() / 255.0\n",
    "                if isinstance(img2, np.ndarray):\n",
    "                    img2 = torch.from_numpy(img2).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "                # 调整大小\n",
    "                img1_small = F.interpolate(img1.unsqueeze(0), size=target_size, mode='bilinear')\n",
    "                img2_small = F.interpolate(img2.unsqueeze(0), size=target_size, mode='bilinear')\n",
    "\n",
    "                # 转移到GPU并清理内存\n",
    "                img1_small = img1_small.to(device)\n",
    "                img2_small = img2_small.to(device)\n",
    "                del img1, img2\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # 执行拼接\n",
    "                result_small, _, _ = model(img1_small, img2_small)\n",
    "\n",
    "                # 清理内存\n",
    "                del img1_small, img2_small\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # 转回原始大小\n",
    "                result = F.interpolate(result_small, size=(1080, 1920), mode='bilinear')\n",
    "                del result_small\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # 转换为numpy\n",
    "                result = result.squeeze(0).cpu().numpy()\n",
    "                result = (result * 255).astype(np.uint8)\n",
    "                result = np.clip(result, 0, 255)\n",
    "                result = np.transpose(result, (1, 2, 0))\n",
    "                result = cv2.cvtColor(result, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "                return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Stitching error: {str(e)}\")\n",
    "        print(f\"Error traceback: {traceback.format_exc()}\")\n",
    "        return None\n",
    "\n",
    "def process_panorama(model, device, total_frames):\n",
    "    \"\"\"改进的渐进式全景图拼接\"\"\"\n",
    "    print(\"开始拼接全景图...\")\n",
    "    panorama = None\n",
    "\n",
    "    try:\n",
    "        for i in range(total_frames - 1):\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            try:\n",
    "                # 读取两帧图像\n",
    "                if panorama is None:\n",
    "                    img1 = cv2.imread(f'temp_frames/frame_{i}.jpg')\n",
    "                    if img1 is None:\n",
    "                        raise ValueError(f\"无法读取帧 {i}\")\n",
    "                else:\n",
    "                    img1 = panorama\n",
    "\n",
    "                img2 = cv2.imread(f'temp_frames/frame_{i+1}.jpg')\n",
    "                if img2 is None:\n",
    "                    raise ValueError(f\"无法读取帧 {i+1}\")\n",
    "\n",
    "                # 转换颜色空间\n",
    "                img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "                img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                print(f\"\\\\nProcessing frames {i} and {i+1}\")\n",
    "                print(f\"Shapes - img1: {img1_rgb.shape}, img2: {img2_rgb.shape}\")\n",
    "\n",
    "                # 执行拼接\n",
    "                result = stitch_with_model(model, img1_rgb, img2_rgb, device)\n",
    "\n",
    "                if result is not None:\n",
    "                    # 保存当前结果作为下一次的全景图输入\n",
    "                    panorama = result\n",
    "\n",
    "                    # 保存中间结果用于调试\n",
    "                    cv2.imwrite(f'temp_frames/intermediate_{i}.jpg', result)\n",
    "\n",
    "                print(f\"Progress: {i+1}/{total_frames-1}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"\\\\nError in frame {i}: {str(e)}\")\n",
    "                print(f\"Error traceback: {traceback.format_exc()}\")\n",
    "                # 如果出错，使用当前帧作为结果\n",
    "                if panorama is None:\n",
    "                    panorama = img1\n",
    "                continue\n",
    "\n",
    "            # 清理内存\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # 保存最终结果\n",
    "        if panorama is not None:\n",
    "            print(\"\\\\nStitching completed\")\n",
    "            cv2.imwrite('output/panorama_final.jpg', panorama)\n",
    "        else:\n",
    "            print(\"\\\\nNo panorama generated\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\nPanorama processing error: {str(e)}\")\n",
    "        if panorama is not None:\n",
    "            cv2.imwrite('output/panorama_error.jpg', panorama)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geometric_consistency_loss(H):\n",
    "    \"\"\"计算几何一致性损失\"\"\"\n",
    "    # 单应性矩阵应该满足的约束\n",
    "    eye = torch.eye(3, device=H.device)\n",
    "    return F.mse_loss(torch.bmm(H, H.transpose(1,2)), eye.expand_as(H))\n",
    "\n",
    "def feature_matching_loss(matches_pyramid):\n",
    "    \"\"\"计算特征匹配一致性损失\"\"\"\n",
    "    loss = 0\n",
    "    for matches in matches_pyramid:\n",
    "        loss += F.binary_cross_entropy(matches, torch.ones_like(matches))\n",
    "    return loss\n",
    "\n",
    "def warp_image(img, H):\n",
    "    \"\"\"使用单应性矩阵变换图像\"\"\"\n",
    "    grid = F.affine_grid(H[:,:2,:], img.size())\n",
    "    return F.grid_sample(img, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 使用更深的网络结构\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        features.append(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        features.append(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        features.append(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        features.append(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "class FeatureMatcher(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1024, 512, 1)  # 512 = 2*256(两个特征连接)\n",
    "        self.conv2 = nn.Conv2d(512, 256, 1)\n",
    "        self.conv3 = nn.Conv2d(256, 128, 1)\n",
    "\n",
    "        # 最终输出匹配得分\n",
    "        self.score = nn.Conv2d(128, 1, 1)\n",
    "\n",
    "    def forward(self, feat1, feat2):\n",
    "        # 连接特征\n",
    "        b, c, h, w = feat1.shape\n",
    "        feat = torch.cat([feat1, feat2], dim=1)\n",
    "\n",
    "        # 计算匹配得分\n",
    "        x = F.relu(self.conv1(feat))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        scores = torch.sigmoid(self.score(x))\n",
    "\n",
    "        return scores\n",
    "\n",
    "class HomographyEstimator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 32, 3, padding=1)\n",
    "        self.fc = nn.Linear(32*8*8, 8)  # 8个参数表示单应性矩阵\n",
    "\n",
    "    def forward(self, matches):\n",
    "        x = F.relu(self.conv1(matches))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        H = self.fc(x)\n",
    "        return H.view(-1, 3, 3)  # 重塑为3x3矩阵\n",
    "    \n",
    "class ImageBlender(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(6, 32, 3, padding=1)  # 6=2*3(两张RGB图像)\n",
    "        self.conv2 = nn.Conv2d(32, 16, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(16, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, img1, warped_img2):\n",
    "        x = torch.cat([img1, warped_img2], dim=1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        weights = torch.sigmoid(self.conv3(x))\n",
    "\n",
    "        return weights * img1 + (1-weights) * warped_img2\n",
    "    \n",
    "class StitchingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = SiameseFeatureExtractor()\n",
    "        self.matcher = FeatureMatcher()\n",
    "        self.homography_estimator = HomographyEstimator()\n",
    "        self.blender = ImageBlender()\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        # 提取特征\n",
    "        feat1_pyramid = self.feature_extractor(img1)\n",
    "        feat2_pyramid = self.feature_extractor(img2)\n",
    "\n",
    "        # 多尺度特征匹配\n",
    "        matches_pyramid = []\n",
    "        for feat1, feat2 in zip(feat1_pyramid, feat2_pyramid):\n",
    "            matches = self.matcher(feat1, feat2)\n",
    "            matches_pyramid.append(matches)\n",
    "\n",
    "        # 估计单应性矩阵\n",
    "        H = self.homography_estimator(matches_pyramid[-1])  # 使用最高层特征\n",
    "\n",
    "        # 变换图像\n",
    "        warped_img2 = warp_image(img2, H)\n",
    "\n",
    "        # 图像融合\n",
    "        result = self.blender(img1, warped_img2)\n",
    "\n",
    "        return result, H, matches_pyramid\n",
    "    \n",
    "class StitchingLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, result, img1, img2, H, matches_pyramid):\n",
    "        # 重建损失\n",
    "        recon_loss = self.l1_loss(result, img2)\n",
    "\n",
    "        # 几何一致性损失\n",
    "        geo_loss = geometric_consistency_loss(H)\n",
    "\n",
    "        # 特征一致性损失\n",
    "        feat_loss = feature_matching_loss(matches_pyramid)\n",
    "\n",
    "        # 平滑度损失\n",
    "        smooth_loss = self.l1_loss(result[:,:,1:,:], result[:,:,:-1,:]) + \\\n",
    "            self.l1_loss(result[:,:,:,1:], result[:,:,:,:-1])\n",
    "\n",
    "        return recon_loss + 0.1 * geo_loss + 0.1 * feat_loss + 0.01 * smooth_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PanoramaNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 特征提取\n",
    "        self.feature_extractor = SiameseFeatureExtractor()\n",
    "        # 特征匹配和单应性估计\n",
    "        self.matcher = FeatureMatcher()\n",
    "        # 变换估计器\n",
    "        self.transform_estimator = HomographyEstimator()\n",
    "        # 融合网络\n",
    "        self.blender = ImageBlender()\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        # 特征提取\n",
    "        feat1 = self.feature_extractor(img1)\n",
    "        feat2 = self.feature_extractor(img2)\n",
    "\n",
    "        # 特征匹配\n",
    "        matches = self.matcher(feat1, feat2)\n",
    "\n",
    "        # 估计变换\n",
    "        H = self.transform_estimator(matches)\n",
    "\n",
    "        # 变换和融合\n",
    "        warped = warp_image(img2, H)\n",
    "        panorama = self.blender(img1, warped)\n",
    "\n",
    "        return panorama, H\n",
    "class SelfSupervisedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, img1, img2, panorama, H):\n",
    "        # 重建损失\n",
    "        recon_loss = StitchingLoss(img1, img2, panorama)\n",
    "\n",
    "        # 几何一致性损失\n",
    "        geo_loss = geometric_consistency_loss(H)\n",
    "\n",
    "        # 特征一致性损失\n",
    "        feat_loss = feature_consistency_loss(img1, img2, panorama)\n",
    "\n",
    "        return recon_loss + geo_loss + feat_loss\n",
    "# 多尺度训练\n",
    "def train_step(model, img1, img2):\n",
    "    # 生成多尺度图像对\n",
    "    img_pairs = generate_multiscale_pairs(img1, img2)\n",
    "\n",
    "    total_loss = 0\n",
    "    for scale_img1, scale_img2 in img_pairs:\n",
    "        # 前向传播\n",
    "        panorama, H = model(scale_img1, scale_img2)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = StitchingLoss(scale_img1, scale_img2, panorama, H)\n",
    "        total_loss += loss\n",
    "\n",
    "    # 反向传播\n",
    "    total_loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "初始化数据集...\n",
      "数据集初始化完成，共有 112 对图像\n",
      "开始训练模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 100/100 [32:31<00:00, 19.52s/it, avg_loss=0.0000, time=19.0s]\n",
      "C:\\Users\\Henry\\AppData\\Local\\Temp\\ipykernel_35704\\2634625010.py:234: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # 使用混合精度\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始拼接全景图...\n",
      "开始拼接全景图...\n",
      "\\nProcessing frames 0 and 1\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 1/112\n",
      "\\nProcessing frames 1 and 2\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 2/112\n",
      "\\nProcessing frames 2 and 3\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 3/112\n",
      "\\nProcessing frames 3 and 4\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 4/112\n",
      "\\nProcessing frames 4 and 5\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 5/112\n",
      "\\nProcessing frames 5 and 6\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 6/112\n",
      "\\nProcessing frames 6 and 7\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 7/112\n",
      "\\nProcessing frames 7 and 8\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 8/112\n",
      "\\nProcessing frames 8 and 9\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 9/112\n",
      "\\nProcessing frames 9 and 10\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 10/112\n",
      "\\nProcessing frames 10 and 11\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 11/112\n",
      "\\nProcessing frames 11 and 12\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 12/112\n",
      "\\nProcessing frames 12 and 13\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 13/112\n",
      "\\nProcessing frames 13 and 14\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 14/112\n",
      "\\nProcessing frames 14 and 15\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 15/112\n",
      "\\nProcessing frames 15 and 16\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 16/112\n",
      "\\nProcessing frames 16 and 17\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 17/112\n",
      "\\nProcessing frames 17 and 18\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 18/112\n",
      "\\nProcessing frames 18 and 19\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 19/112\n",
      "\\nProcessing frames 19 and 20\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 20/112\n",
      "\\nProcessing frames 20 and 21\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 21/112\n",
      "\\nProcessing frames 21 and 22\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 22/112\n",
      "\\nProcessing frames 22 and 23\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 23/112\n",
      "\\nProcessing frames 23 and 24\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 24/112\n",
      "\\nProcessing frames 24 and 25\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 25/112\n",
      "\\nProcessing frames 25 and 26\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 26/112\n",
      "\\nProcessing frames 26 and 27\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 27/112\n",
      "\\nProcessing frames 27 and 28\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 28/112\n",
      "\\nProcessing frames 28 and 29\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 29/112\n",
      "\\nProcessing frames 29 and 30\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 30/112\n",
      "\\nProcessing frames 30 and 31\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 31/112\n",
      "\\nProcessing frames 31 and 32\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 32/112\n",
      "\\nProcessing frames 32 and 33\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 33/112\n",
      "\\nProcessing frames 33 and 34\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 34/112\n",
      "\\nProcessing frames 34 and 35\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 35/112\n",
      "\\nProcessing frames 35 and 36\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 36/112\n",
      "\\nProcessing frames 36 and 37\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 37/112\n",
      "\\nProcessing frames 37 and 38\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 38/112\n",
      "\\nProcessing frames 38 and 39\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 39/112\n",
      "\\nProcessing frames 39 and 40\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 40/112\n",
      "\\nProcessing frames 40 and 41\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 41/112\n",
      "\\nProcessing frames 41 and 42\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 42/112\n",
      "\\nProcessing frames 42 and 43\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 43/112\n",
      "\\nProcessing frames 43 and 44\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 44/112\n",
      "\\nProcessing frames 44 and 45\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 45/112\n",
      "\\nProcessing frames 45 and 46\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 46/112\n",
      "\\nProcessing frames 46 and 47\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 47/112\n",
      "\\nProcessing frames 47 and 48\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 48/112\n",
      "\\nProcessing frames 48 and 49\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 49/112\n",
      "\\nProcessing frames 49 and 50\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 50/112\n",
      "\\nProcessing frames 50 and 51\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 51/112\n",
      "\\nProcessing frames 51 and 52\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 52/112\n",
      "\\nProcessing frames 52 and 53\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 53/112\n",
      "\\nProcessing frames 53 and 54\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 54/112\n",
      "\\nProcessing frames 54 and 55\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 55/112\n",
      "\\nProcessing frames 55 and 56\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 56/112\n",
      "\\nProcessing frames 56 and 57\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 57/112\n",
      "\\nProcessing frames 57 and 58\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 58/112\n",
      "\\nProcessing frames 58 and 59\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 59/112\n",
      "\\nProcessing frames 59 and 60\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 60/112\n",
      "\\nProcessing frames 60 and 61\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 61/112\n",
      "\\nProcessing frames 61 and 62\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 62/112\n",
      "\\nProcessing frames 62 and 63\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 63/112\n",
      "\\nProcessing frames 63 and 64\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 64/112\n",
      "\\nProcessing frames 64 and 65\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 65/112\n",
      "\\nProcessing frames 65 and 66\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 66/112\n",
      "\\nProcessing frames 66 and 67\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 67/112\n",
      "\\nProcessing frames 67 and 68\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 68/112\n",
      "\\nProcessing frames 68 and 69\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 69/112\n",
      "\\nProcessing frames 69 and 70\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 70/112\n",
      "\\nProcessing frames 70 and 71\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 71/112\n",
      "\\nProcessing frames 71 and 72\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 72/112\n",
      "\\nProcessing frames 72 and 73\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 73/112\n",
      "\\nProcessing frames 73 and 74\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 74/112\n",
      "\\nProcessing frames 74 and 75\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 75/112\n",
      "\\nProcessing frames 75 and 76\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 76/112\n",
      "\\nProcessing frames 76 and 77\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 77/112\n",
      "\\nProcessing frames 77 and 78\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 78/112\n",
      "\\nProcessing frames 78 and 79\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 79/112\n",
      "\\nProcessing frames 79 and 80\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 80/112\n",
      "\\nProcessing frames 80 and 81\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 81/112\n",
      "\\nProcessing frames 81 and 82\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 82/112\n",
      "\\nProcessing frames 82 and 83\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 83/112\n",
      "\\nProcessing frames 83 and 84\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 84/112\n",
      "\\nProcessing frames 84 and 85\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 85/112\n",
      "\\nProcessing frames 85 and 86\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 86/112\n",
      "\\nProcessing frames 86 and 87\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 87/112\n",
      "\\nProcessing frames 87 and 88\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 88/112\n",
      "\\nProcessing frames 88 and 89\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 89/112\n",
      "\\nProcessing frames 89 and 90\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 90/112\n",
      "\\nProcessing frames 90 and 91\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 91/112\n",
      "\\nProcessing frames 91 and 92\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 92/112\n",
      "\\nProcessing frames 92 and 93\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 93/112\n",
      "\\nProcessing frames 93 and 94\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 94/112\n",
      "\\nProcessing frames 94 and 95\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 95/112\n",
      "\\nProcessing frames 95 and 96\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 96/112\n",
      "\\nProcessing frames 96 and 97\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 97/112\n",
      "\\nProcessing frames 97 and 98\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 98/112\n",
      "\\nProcessing frames 98 and 99\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 99/112\n",
      "\\nProcessing frames 99 and 100\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 100/112\n",
      "\\nProcessing frames 100 and 101\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 101/112\n",
      "\\nProcessing frames 101 and 102\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 102/112\n",
      "\\nProcessing frames 102 and 103\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 103/112\n",
      "\\nProcessing frames 103 and 104\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 104/112\n",
      "\\nProcessing frames 104 and 105\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 105/112\n",
      "\\nProcessing frames 105 and 106\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 106/112\n",
      "\\nProcessing frames 106 and 107\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 107/112\n",
      "\\nProcessing frames 107 and 108\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 108/112\n",
      "\\nProcessing frames 108 and 109\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 109/112\n",
      "\\nProcessing frames 109 and 110\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 110/112\n",
      "\\nProcessing frames 110 and 111\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 111/112\n",
      "\\nProcessing frames 111 and 112\n",
      "Shapes - img1: (1080, 1920, 3), img2: (1080, 1920, 3)\n",
      "Progress: 112/112\n",
      "\\nStitching completed\n"
     ]
    }
   ],
   "source": [
    "def main(num_epochs):\n",
    "    \"\"\"主函数\"\"\"\n",
    "    try:\n",
    "        # 检查CUDA是否可用\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        \n",
    "        # 创建数据集和数据加载器，使用更保守的设置\n",
    "        try:\n",
    "            print(\"初始化数据集...\")\n",
    "            dataset = FramePairsDataset('temp_frames', size=(128, 128))\n",
    "            \n",
    "            # 使用单进程模式加载数据\n",
    "            train_loader = DataLoader(\n",
    "                dataset, \n",
    "                batch_size=2,\n",
    "                shuffle=True,\n",
    "                num_workers=0,  # 使用单进程\n",
    "                pin_memory=False\n",
    "                # True if torch.cuda.is_available() else False\n",
    "            )\n",
    "            print(f\"数据集初始化完成，共有 {len(dataset)} 对图像\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"创建数据加载器时出错: {str(e)}\")\n",
    "            raise\n",
    "        \n",
    "        # 创建模型\n",
    "        model = StitchingNet()\n",
    "        \n",
    "        # 训练模型\n",
    "        print(\"开始训练模型...\")\n",
    "        train_model(model, train_loader, num_epochs, device=device)\n",
    "        \n",
    "        # 使用训练好的模型进行拼接\n",
    "        print(\"开始拼接全景图...\")\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                              std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        process_panorama(model, device, total_frames)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"发生错误: {str(e)}\")\n",
    "    finally:\n",
    "        # cleanup()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_epochs = 100\n",
    "    main(num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
