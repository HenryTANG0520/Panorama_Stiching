{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自监督学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 982 frame sequences from video\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "class DepthNet(nn.Module):\n",
    "    \"\"\"优化后的深度估计网络\"\"\"\n",
    "    def __init__(self, training=False):\n",
    "        super(DepthNet, self).__init__()\n",
    "        self.training = training\n",
    "        \n",
    "        # 基础编码器-解码器(推理和训练都会使用)\n",
    "        self.encoder = nn.ModuleList([\n",
    "            self._make_encoder_layer(3, 32),      # 1/2\n",
    "            self._make_encoder_layer(32, 64),     # 1/4\n",
    "            self._make_encoder_layer(64, 128),    # 1/8\n",
    "        ])\n",
    "        \n",
    "        self.decoder = nn.ModuleList([\n",
    "            self._make_decoder_layer(128, 64),    # 1/4\n",
    "            self._make_decoder_layer(64, 32),     # 1/2\n",
    "            self._make_decoder_layer(32, 1),      # 1\n",
    "        ])\n",
    "        \n",
    "        # 训练时的额外组件\n",
    "        if training:\n",
    "            self.auxiliary_encoder = nn.ModuleList([\n",
    "                self._make_encoder_layer(128, 256),   # 1/16\n",
    "                self._make_encoder_layer(256, 512),   # 1/32\n",
    "            ])\n",
    "            self.auxiliary_decoder = nn.ModuleList([\n",
    "                self._make_decoder_layer(512, 256),   # 1/16\n",
    "                self._make_decoder_layer(256, 128),   # 1/8\n",
    "            ])\n",
    "            \n",
    "    def _make_encoder_layer(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def _make_decoder_layer(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, 3, stride=2, \n",
    "                             padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 基础特征提取(推理和训练共用)\n",
    "        features = []\n",
    "        for encoder in self.encoder:\n",
    "            x = encoder(x)\n",
    "            features.append(x)\n",
    "            \n",
    "        if not self.training:\n",
    "            # 推理时只使用基础解码器\n",
    "            for i, decoder in enumerate(self.decoder):\n",
    "                if i < len(self.decoder) - 1:\n",
    "                    x = decoder(x + features[-(i+2)])\n",
    "                else:\n",
    "                    x = decoder(x)\n",
    "            return torch.sigmoid(x)\n",
    "            \n",
    "        # 训练时使用额外的处理\n",
    "        aux_features = []\n",
    "        for aux_encoder in self.auxiliary_encoder:\n",
    "            x = aux_encoder(x)\n",
    "            aux_features.append(x)\n",
    "            \n",
    "        # 解码时合并所有特征\n",
    "        for i, aux_decoder in enumerate(self.auxiliary_decoder):\n",
    "            x = aux_decoder(x + aux_features[-(i+1)])\n",
    "            \n",
    "        for i, decoder in enumerate(self.decoder):\n",
    "            if i < len(self.decoder) - 1:\n",
    "                x = decoder(x + features[-(i+2)])\n",
    "            else:\n",
    "                x = decoder(x)\n",
    "                \n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "class PoseNet(nn.Module):\n",
    "    \"\"\"优化后的位姿估计网络\"\"\"\n",
    "    def __init__(self, training=False):\n",
    "        super(PoseNet, self).__init__()\n",
    "        self.training = training\n",
    "        \n",
    "        # 基础位姿估计(推理和训练共用)\n",
    "        self.base_encoder = nn.Sequential(\n",
    "            self._make_layer(6, 32),    # 输入是concat的两帧\n",
    "            self._make_layer(32, 64),\n",
    "            self._make_layer(64, 128),\n",
    "        )\n",
    "        \n",
    "        self.base_pose_pred = nn.Conv2d(128, 6, 1)\n",
    "        \n",
    "        # 训练时的额外组件\n",
    "        if training:\n",
    "            self.aux_encoder = nn.Sequential(\n",
    "                self._make_layer(128, 256),\n",
    "                self._make_layer(256, 512),\n",
    "            )\n",
    "            self.aux_pose_pred = nn.Conv2d(512, 6, 1)\n",
    "            \n",
    "    def _make_layer(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, target_frame, source_frame):\n",
    "        x = torch.cat([target_frame, source_frame], dim=1)\n",
    "        \n",
    "        # 基础位姿估计\n",
    "        feat = self.base_encoder(x)\n",
    "        base_feat = F.adaptive_avg_pool2d(feat, 1)\n",
    "        base_pose = self.base_pose_pred(base_feat)\n",
    "        \n",
    "        if not self.training:\n",
    "            return base_pose.view(-1, 6)\n",
    "            \n",
    "        # 训练时使用额外特征\n",
    "        aux_feat = self.aux_encoder(feat)\n",
    "        aux_feat = F.adaptive_avg_pool2d(aux_feat, 1)\n",
    "        aux_pose = self.aux_pose_pred(aux_feat)\n",
    "        \n",
    "        # 融合基础和辅助预测\n",
    "        final_pose = (base_pose + aux_pose) / 2\n",
    "        return final_pose.view(-1, 6)\n",
    "\n",
    "class SelfSupervisedTrainer:\n",
    "    def __init__(self, height=256, width=384):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        # 初始化网络(训练模式)\n",
    "        self.depth_net = DepthNet(training=True).to(self.device)\n",
    "        self.pose_net = PoseNet(training=True).to(self.device)\n",
    "        \n",
    "        # 使用混合精度训练\n",
    "        self.scaler = torch.amp.GradScaler('cuda')\n",
    "        \n",
    "        # 优化器\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.depth_net.parameters(), 'lr': 1e-4},\n",
    "            {'params': self.pose_net.parameters(), 'lr': 1e-4}\n",
    "        ])\n",
    "        \n",
    "        # 学习率调度器\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', factor=0.5, patience=5\n",
    "        )\n",
    "        \n",
    "    def train(self, video_path, num_epochs=50, batch_size=4):\n",
    "        # 增强的数据预处理\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        ])\n",
    "        \n",
    "        dataset = VideoDataset(video_path, transform=transform,\n",
    "                             max_frames=2000,\n",
    "                             target_size=(self.width, self.height))\n",
    "        \n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            self.depth_net.train()\n",
    "            self.pose_net.train()\n",
    "            \n",
    "            for batch_idx, (frame1, frame2) in enumerate(dataloader):\n",
    "                frame1 = frame1.to(self.device, non_blocking=True)\n",
    "                frame2 = frame2.to(self.device, non_blocking=True)\n",
    "                \n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    # 前向传播\n",
    "                    depth1 = self.depth_net(frame1)\n",
    "                    depth2 = self.depth_net(frame2)\n",
    "                    pose = self.pose_net(frame1, frame2)\n",
    "                    \n",
    "                    # 训练时使用更复杂的损失函数\n",
    "                    loss = (\n",
    "                        self.photometric_loss(frame1, frame2, depth1, depth2, pose) +\n",
    "                        0.1 * self.smoothness_loss(depth1, frame1) +\n",
    "                        0.05 * self.geometric_consistency_loss(depth1, depth2, pose)\n",
    "                    )\n",
    "                \n",
    "                self.optimizer.zero_grad(set_to_none=True)\n",
    "                self.scaler.scale(loss).backward()\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    avg_loss = total_loss / (batch_idx + 1)\n",
    "                    print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, '\n",
    "                          f'Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            epoch_loss = total_loss / len(dataloader)\n",
    "            self.scheduler.step(epoch_loss)\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                self.save_models(f'checkpoint_epoch_{epoch+1}')\n",
    "\n",
    "    def geometric_consistency_loss(self, depth1, depth2, pose):\n",
    "        \"\"\"几何一致性损失\"\"\"\n",
    "        # 计算重投影后的深度一致性\n",
    "        proj_depth2 = self.warp_depth(depth2, depth1, pose)\n",
    "        consistency_mask = self.compute_consistency_mask(depth1, proj_depth2)\n",
    "        \n",
    "        loss = torch.abs(depth1 - proj_depth2) * consistency_mask\n",
    "        return torch.mean(loss)\n",
    "        \n",
    "    def warp_depth(self, depth2, depth1, pose):\n",
    "        \"\"\"将depth2变换到depth1的视角\"\"\"\n",
    "        batch_size = depth1.size(0)\n",
    "        height = depth1.size(2)\n",
    "        width = depth1.size(3)\n",
    "        \n",
    "        # 生成像素网格\n",
    "        pixels_x, pixels_y = torch.meshgrid(\n",
    "            torch.arange(width, device=self.device),\n",
    "            torch.arange(height, device=self.device)\n",
    "        )\n",
    "        pixels = torch.stack([pixels_x, pixels_y, torch.ones_like(pixels_x)]).float()\n",
    "        \n",
    "        # 投影变换\n",
    "        K = self.get_camera_matrix(height, width).to(self.device)\n",
    "        pixels_3d = torch.matmul(torch.inverse(K), pixels.reshape(3, -1))\n",
    "        pixels_3d = pixels_3d.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # 应用深度\n",
    "        points_3d = pixels_3d * depth2.reshape(batch_size, 1, -1)\n",
    "        \n",
    "        # 应用位姿变换\n",
    "        R = pose[:, :3].reshape(-1, 3, 3)\n",
    "        t = pose[:, 3:].reshape(-1, 3, 1)\n",
    "        transformed_points = torch.matmul(R, points_3d) + t\n",
    "        \n",
    "        # 投影回图像平面\n",
    "        proj_points = torch.matmul(K, transformed_points)\n",
    "        proj_pixels = proj_points[:, :2] / (proj_points[:, 2:] + 1e-7)\n",
    "        \n",
    "        # 网格采样\n",
    "        proj_pixels = proj_pixels.reshape(batch_size, 2, height, width)\n",
    "        proj_pixels = proj_pixels.permute(0, 2, 3, 1)\n",
    "        warped_depth = F.grid_sample(depth1, proj_pixels, align_corners=True)\n",
    "        \n",
    "        return warped_depth\n",
    "        \n",
    "    def compute_consistency_mask(self, depth1, proj_depth2, threshold=0.1):\n",
    "        \"\"\"计算深度一致性掩码\"\"\"\n",
    "        diff = torch.abs(depth1 - proj_depth2)\n",
    "        mask = (diff < threshold).float()\n",
    "        return mask\n",
    "\n",
    "    def save_inference_model(self, output_dir):\n",
    "        \"\"\"保存推理模型\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 创建推理模型实例\n",
    "        inference_depth_net = DepthNet(training=False)\n",
    "        inference_pose_net = PoseNet(training=False)\n",
    "        \n",
    "        # 复制基础组件的权重\n",
    "        depth_state_dict = self.depth_net.state_dict()\n",
    "        pose_state_dict = self.pose_net.state_dict()\n",
    "        \n",
    "        # 只保留基础组件的权重\n",
    "        inference_depth_dict = {k: v for k, v in depth_state_dict.items() \n",
    "                              if not k.startswith('auxiliary')}\n",
    "        inference_pose_dict = {k: v for k, v in pose_state_dict.items() \n",
    "                             if not k.startswith('aux')}\n",
    "        \n",
    "        # 加载权重到推理模型\n",
    "        inference_depth_net.load_state_dict(inference_depth_dict)\n",
    "        inference_pose_net.load_state_dict(inference_pose_dict)\n",
    "        \n",
    "        # 将模型移到CPU并设置为评估模式\n",
    "        inference_depth_net = inference_depth_net.cpu().eval()\n",
    "        inference_pose_net = inference_pose_net.cpu().eval()\n",
    "        \n",
    "        # 保存完整的推理模型\n",
    "        torch.save(inference_depth_net, output_path / 'inference_depth_net.pth')\n",
    "        torch.save(inference_pose_net, output_path / 'inference_pose_net.pth')\n",
    "\n",
    "    @staticmethod\n",
    "    def load_inference_models(model_dir):\n",
    "        \"\"\"加载推理模型\"\"\"\n",
    "        model_path = Path(model_dir)\n",
    "        depth_net = torch.load(model_path / 'inference_depth_net.pth')\n",
    "        pose_net = torch.load(model_path / 'inference_pose_net.pth')\n",
    "        return depth_net, pose_net\n",
    "\n",
    "    def generate_depth_map(self, frame, inference_depth_net=None):\n",
    "        \"\"\"推理时生成深度图\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 如果没有提供推理模型，使用训练模型\n",
    "            if inference_depth_net is None:\n",
    "                self.depth_net.eval()\n",
    "                self.depth_net.training = False\n",
    "                model = self.depth_net\n",
    "            else:\n",
    "                model = inference_depth_net\n",
    "                \n",
    "            # 预处理帧\n",
    "            frame_tensor = transforms.ToTensor()(frame).unsqueeze(0)\n",
    "            frame_tensor = transforms.Resize((self.height, self.width))(frame_tensor)\n",
    "            frame_tensor = transforms.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                                             std=[0.5, 0.5, 0.5])(frame_tensor)\n",
    "                                             \n",
    "            # 如果使用训练模型，需要移到正确的设备上\n",
    "            if inference_depth_net is None:\n",
    "                frame_tensor = frame_tensor.to(self.device)\n",
    "            \n",
    "            depth = model(frame_tensor)\n",
    "            return depth.cpu().numpy()[0, 0]\n",
    "            \n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"优化后的视频数据集类\"\"\"\n",
    "    def __init__(self, video_path, transform=None, max_frames=None, target_size=(256, 384),\n",
    "                 sequence_length=3):  # 新增sequence_length参数\n",
    "        self.frames = []\n",
    "        self.transform = transform\n",
    "        self.target_size = target_size\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # 读取视频帧\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        \n",
    "        # 使用双端队列存储帧,便于滑动窗口访问\n",
    "        from collections import deque\n",
    "        frame_buffer = deque(maxlen=sequence_length)\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret or (max_frames and frame_count >= max_frames):\n",
    "                break\n",
    "                \n",
    "            # 预处理\n",
    "            frame = cv2.resize(frame, target_size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            frame_buffer.append(frame)\n",
    "            \n",
    "            # 当缓冲区填满时,保存帧序列\n",
    "            if len(frame_buffer) == sequence_length:\n",
    "                self.frames.append(list(frame_buffer))\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            # 每隔几帧采样一次\n",
    "            if frame_count % 2 == 0:\n",
    "                cap.grab()\n",
    "                \n",
    "        cap.release()\n",
    "        print(f\"Loaded {len(self.frames)} frame sequences from video\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        frame_sequence = self.frames[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            # 对整个序列应用相同的随机变换\n",
    "            if random.random() > 0.5:\n",
    "                frame_sequence = [np.fliplr(frame) for frame in frame_sequence]\n",
    "            \n",
    "            # 应用颜色增强,但保持序列一致性\n",
    "            color_jitter = transforms.ColorJitter(\n",
    "                brightness=0.2,\n",
    "                contrast=0.2,\n",
    "                saturation=0.2,\n",
    "                hue=0.1\n",
    "            )\n",
    "            \n",
    "            frame_sequence = [self.transform(frame) for frame in frame_sequence]\n",
    "            \n",
    "            if random.random() > 0.5:\n",
    "                frame_sequence = [color_jitter(frame) for frame in frame_sequence]\n",
    "        \n",
    "        return tuple(frame_sequence)\n",
    "\n",
    "class ViewConfidenceEstimator:\n",
    "    \"\"\"优化后的视角置信度估计器\"\"\"\n",
    "    def __init__(self, threshold_angle=45.0):\n",
    "        self.threshold_angle = threshold_angle\n",
    "        \n",
    "    @torch.no_grad()  # 推理时优化\n",
    "    def compute_view_angles(self, depth_map, K, pose):\n",
    "        \"\"\"计算每个像素的观察角度\"\"\"\n",
    "        device = depth_map.device\n",
    "        batch_size, _, height, width = depth_map.shape\n",
    "        \n",
    "        # 使用缓存的网格坐标\n",
    "        if not hasattr(self, 'pixel_grid'):\n",
    "            y_coords, x_coords = torch.meshgrid(\n",
    "                torch.arange(height, device=device),\n",
    "                torch.arange(width, device=device)\n",
    "            )\n",
    "            pixels = torch.stack([x_coords, y_coords, torch.ones_like(x_coords)]).float()\n",
    "            self.pixel_grid = pixels.reshape(3, -1).to(device)\n",
    "        \n",
    "        # 计算3D点\n",
    "        depths = depth_map.reshape(batch_size, -1, 1)\n",
    "        points_3d = (torch.inverse(K.to(device)) @ self.pixel_grid).unsqueeze(0) * depths\n",
    "        \n",
    "        # 计算观察角度\n",
    "        view_directions = F.normalize(points_3d, p=2, dim=1)\n",
    "        camera_normal = pose[:, :3, 2]  # 相机朝向\n",
    "        \n",
    "        # 批量计算点积\n",
    "        angles = torch.arccos(torch.clamp(\n",
    "            torch.sum(view_directions * camera_normal.unsqueeze(1), dim=1),\n",
    "            -1.0, 1.0\n",
    "        ))\n",
    "        \n",
    "        return angles.reshape(batch_size, 1, height, width)\n",
    "    \n",
    "    @torch.no_grad()  # 推理时优化\n",
    "    def get_confidence_mask(self, view_angles):\n",
    "        \"\"\"根据视角生成置信度掩码\"\"\"\n",
    "        angles_deg = view_angles * 180.0 / np.pi\n",
    "        confidence = torch.exp(-(angles_deg / self.threshold_angle) ** 2)\n",
    "        return confidence\n",
    "\n",
    "class DSMGenerator:\n",
    "    \"\"\"优化后的数字表面模型生成器\"\"\"\n",
    "    def __init__(self, cell_size=0.1, grid_size=(1000, 1000)):\n",
    "        self.cell_size = cell_size\n",
    "        self.grid_size = grid_size\n",
    "        self.dsm = np.zeros(grid_size)\n",
    "        self.weight_sum = np.zeros(grid_size)\n",
    "        \n",
    "    @torch.no_grad()  # 推理时优化\n",
    "    def update_dsm(self, depth_map, pose, confidence_map=None, K=None):\n",
    "        \"\"\"更新DSM\"\"\"\n",
    "        # 转换深度图到点云\n",
    "        points_3d = self.depth_to_points(depth_map, K)\n",
    "        \n",
    "        # 应用位姿变换\n",
    "        points_world = self.transform_points(points_3d, pose)\n",
    "        \n",
    "        # 投影到网格\n",
    "        grid_coords = self.project_to_grid(points_world)\n",
    "        \n",
    "        # 更新DSM\n",
    "        valid_mask = (\n",
    "            (grid_coords[:, 0] >= 0) & \n",
    "            (grid_coords[:, 0] < self.grid_size[0]) &\n",
    "            (grid_coords[:, 1] >= 0) & \n",
    "            (grid_coords[:, 1] < self.grid_size[1])\n",
    "        )\n",
    "        \n",
    "        grid_coords = grid_coords[valid_mask]\n",
    "        heights = points_world[valid_mask, 2]\n",
    "        weights = confidence_map.reshape(-1)[valid_mask] if confidence_map is not None \\\n",
    "                 else np.ones_like(heights)\n",
    "        \n",
    "        # 使用numpy的高效索引操作\n",
    "        np.add.at(self.dsm, (grid_coords[:, 0], grid_coords[:, 1]), \n",
    "                 heights * weights)\n",
    "        np.add.at(self.weight_sum, (grid_coords[:, 0], grid_coords[:, 1]), \n",
    "                 weights)\n",
    "    \n",
    "    def finalize_dsm(self):\n",
    "        \"\"\"完成DSM生成\"\"\"\n",
    "        # 避免除零\n",
    "        mask = self.weight_sum > 0\n",
    "        self.dsm[mask] /= self.weight_sum[mask]\n",
    "        \n",
    "        # 填充未观测区域\n",
    "        self._fill_unobserved_regions()\n",
    "        \n",
    "        return self.dsm\n",
    "        \n",
    "    @staticmethod\n",
    "    def depth_to_points(depth_map, K):\n",
    "        \"\"\"深度图转换为点云\"\"\"\n",
    "        height, width = depth_map.shape\n",
    "        y_coords, x_coords = np.meshgrid(np.arange(height), np.arange(width), \n",
    "                                       indexing='ij')\n",
    "        pixels = np.stack([x_coords, y_coords, np.ones_like(x_coords)]).reshape(3, -1)\n",
    "        \n",
    "        # 反投影\n",
    "        points_3d = np.linalg.inv(K) @ pixels\n",
    "        points_3d *= depth_map.reshape(1, -1)\n",
    "        \n",
    "        return points_3d.T\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform_points(points_3d, pose):\n",
    "        \"\"\"应用位姿变换\"\"\"\n",
    "        R = pose[:3, :3]\n",
    "        t = pose[:3, 3]\n",
    "        return (R @ points_3d.T).T + t\n",
    "    \n",
    "    def project_to_grid(self, points_world):\n",
    "        \"\"\"投影到网格坐标\"\"\"\n",
    "        grid_coords = np.floor(points_world[:, :2] / self.cell_size).astype(int)\n",
    "        return grid_coords\n",
    "    \n",
    "    def _fill_unobserved_regions(self):\n",
    "        \"\"\"填充未观测区域(使用最近邻插值)\"\"\"\n",
    "        from scipy.ndimage import distance_transform_edt\n",
    "        \n",
    "        unobserved = self.weight_sum == 0\n",
    "        if not np.any(unobserved):\n",
    "            return\n",
    "            \n",
    "        # 计算到最近有效点的距离\n",
    "        dist = distance_transform_edt(unobserved)\n",
    "        \n",
    "        # 创建掩码\n",
    "        mask = dist > 0\n",
    "        \n",
    "        # 使用最近邻填充\n",
    "        y_indices, x_indices = np.nonzero(unobserved)\n",
    "        valid_y, valid_x = np.nonzero(~unobserved)\n",
    "        \n",
    "        if len(valid_y) == 0:\n",
    "            return\n",
    "            \n",
    "        from scipy.spatial import cKDTree\n",
    "        tree = cKDTree(np.column_stack([valid_y, valid_x]))\n",
    "        \n",
    "        # 找到最近的有效点\n",
    "        _, indices = tree.query(np.column_stack([y_indices, x_indices]))\n",
    "        \n",
    "        # 填充值\n",
    "        self.dsm[y_indices, x_indices] = self.dsm[valid_y[indices], valid_x[indices]]\n",
    "        self.weight_sum[unobserved] = 1  # 标记为已填充\n",
    "\n",
    "def main(video_path, num_epochs):\n",
    "    \"\"\"主函数\"\"\"\n",
    "    output_dir = \"output\"\n",
    "    \n",
    "    # 创建训练器\n",
    "    trainer = SelfSupervisedTrainer()\n",
    "    \n",
    "    # 训练模型\n",
    "    trainer.train(video_path, num_epochs, batch_size=4)\n",
    "    \n",
    "    # 保存训练模型和推理模型\n",
    "    trainer.save_inference_model(output_dir)\n",
    "    \n",
    "    # 加载推理模型\n",
    "    inference_depth_net, inference_pose_net = trainer.load_inference_models(output_dir)\n",
    "    \n",
    "    # 使用推理模型生成DSM\n",
    "    dsm_generator = DSMGenerator()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    prev_frame = None\n",
    "\n",
    "    pose = np.eye(4)  # 初始位姿\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # 使用推理模型生成深度图\n",
    "        depth = trainer.generate_depth_map(frame, inference_depth_net)\n",
    "        \n",
    "        if prev_frame is not None:\n",
    "            # 使用推理模型估计位姿\n",
    "            with torch.no_grad():\n",
    "                pose_update = inference_pose_net(\n",
    "                    trainer.preprocess_frame(frame),\n",
    "                    trainer.preprocess_frame(prev_frame)\n",
    "                ).cpu().numpy()\n",
    "                \n",
    "                # 更新累积位姿\n",
    "                pose = update_pose(pose, pose_update)\n",
    "            \n",
    "            # 更新DSM\n",
    "            dsm_generator.update_dsm(depth, pose)\n",
    "        \n",
    "        prev_frame = frame.copy()\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # 完成DSM生成\n",
    "    final_dsm = dsm_generator.finalize_dsm()\n",
    "    \n",
    "    # 保存结果\n",
    "    np.save(Path(output_dir) / 'dsm.npy', final_dsm)\n",
    "    \n",
    "    # 可视化\n",
    "    visualize_dsm(final_dsm, Path(output_dir) / 'dsm_visualization.png')\n",
    "\n",
    "def update_pose(prev_pose, pose_update):\n",
    "    \"\"\"更新累积位姿\"\"\"\n",
    "    R_update = cv2.Rodrigues(pose_update[:3])[0]\n",
    "    t_update = pose_update[3:]\n",
    "    \n",
    "    # 合并旋转和平移\n",
    "    R = prev_pose[:3, :3] @ R_update\n",
    "    t = prev_pose[:3, :3] @ t_update + prev_pose[:3, 3]\n",
    "    \n",
    "    new_pose = np.eye(4)\n",
    "    new_pose[:3, :3] = R\n",
    "    new_pose[:3, 3] = t\n",
    "    \n",
    "    return new_pose\n",
    "\n",
    "def visualize_dsm(dsm, output_path):\n",
    "    \"\"\"可视化DSM\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 主视图\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(dsm, cmap='terrain')\n",
    "    plt.colorbar(label='Height (m)')\n",
    "    plt.title('Digital Surface Model')\n",
    "    \n",
    "    # 等高线图\n",
    "    plt.subplot(122)\n",
    "    plt.contour(dsm, levels=20, cmap='viridis')\n",
    "    plt.colorbar(label='Height (m)')\n",
    "    plt.title('Contour Map')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"video/7.mp4\"\n",
    "    num_epochs=50\n",
    "    main(video_path, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
