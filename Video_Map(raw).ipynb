{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# 1. Patch Embedding and Position Encoding\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c'),\n",
    "        )\n",
    "        \n",
    "        # Learnable position embeddings\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # Add cls token and position embeddings\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=B)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        return x\n",
    "\n",
    "# 2. Multi-Head Self Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "# 3. Transformer Encoder Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# 4. Vision Transformer Feature Extractor\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, qkv_bias, drop_rate, attn_drop_rate)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "# 5. Cross Attention Matching Module\n",
    "class CrossAttentionMatcher(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert dim % num_heads == 0\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        B, N, C = x1.shape\n",
    "        q = self.q_proj(x1).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.k_proj(x2).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.v_proj(x2).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        # Return both attention weights and values\n",
    "        return x, attn\n",
    "\n",
    "# 6. Complete Image Stitching Transformer\n",
    "class ImageStitchingTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = ViTFeatureExtractor(\n",
    "            img_size, patch_size, in_channels, embed_dim, depth, \n",
    "            num_heads, mlp_ratio, qkv_bias, drop_rate, attn_drop_rate\n",
    "        )\n",
    "        self.matcher = CrossAttentionMatcher(embed_dim, num_heads, attn_drop_rate, drop_rate)\n",
    "        \n",
    "        # Matching head\n",
    "        self.match_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        # Extract features\n",
    "        feat1 = self.feature_extractor(img1)\n",
    "        feat2 = self.feature_extractor(img2)\n",
    "        \n",
    "        # Cross attention matching\n",
    "        matched_features, attention_weights = self.matcher(feat1, feat2)\n",
    "        \n",
    "        # Generate matching scores\n",
    "        matching_scores = self.match_head(matched_features).squeeze(-1)\n",
    "        \n",
    "        return {\n",
    "            'features1': feat1,\n",
    "            'features2': feat2,\n",
    "            'matched_features': matched_features,\n",
    "            'attention_weights': attention_weights,\n",
    "            'matching_scores': matching_scores\n",
    "        }\n",
    "\n",
    "# 7. Training utilities\n",
    "def matching_loss(pred_scores, gt_matches, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Compute matching loss using cross entropy\n",
    "    pred_scores: predicted matching scores [B, N]\n",
    "    gt_matches: ground truth matching indices [B, N]\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(pred_scores / temperature, gt_matches)\n",
    "\n",
    "# Example usage\n",
    "def train_step(model, optimizer, img1, img2, gt_matches):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(img1, img2)\n",
    "    loss = matching_loss(outputs['matching_scores'], gt_matches)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Testing/inference function\n",
    "@torch.no_grad()\n",
    "def find_matches(model, img1, img2, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Find matches between two images\n",
    "    Returns: matched point pairs and confidence scores\n",
    "    \"\"\"\n",
    "    outputs = model(img1, img2)\n",
    "    scores = torch.sigmoid(outputs['matching_scores'])\n",
    "    matches = scores > threshold\n",
    "    \n",
    "    # Get patch coordinates for matched points\n",
    "    matched_indices = torch.nonzero(matches).cpu().numpy()\n",
    "    confidence_scores = scores[matches].cpu().numpy()\n",
    "    \n",
    "    return matched_indices, confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "class VideoFrameExtractor:\n",
    "    \"\"\"Extract frames from video for stitching\"\"\"\n",
    "    def __init__(self, overlap_ratio=0.3, frame_interval=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            overlap_ratio (float): Expected overlap between consecutive frames\n",
    "            frame_interval (int): Number of frames to skip\n",
    "        \"\"\"\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.frame_interval = frame_interval\n",
    "        \n",
    "    def extract_frames(self, video_path, output_dir):\n",
    "        \"\"\"\n",
    "        Extract frames from video and save to directory\n",
    "        \n",
    "        Args:\n",
    "            video_path (str): Path to input video\n",
    "            output_dir (str): Directory to save extracted frames\n",
    "        \n",
    "        Returns:\n",
    "            list: Paths to extracted frame pairs\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"Error opening video file: {video_path}\")\n",
    "            \n",
    "        # Create output directory\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        frame_pairs = []\n",
    "        frame_count = 0\n",
    "        last_saved_frame = None\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if frame_count % self.frame_interval == 0:\n",
    "                frame_path = output_dir / f\"frame_{frame_count:06d}.jpg\"\n",
    "                cv2.imwrite(str(frame_path), frame)\n",
    "                \n",
    "                if last_saved_frame is not None:\n",
    "                    frame_pairs.append((last_saved_frame, frame_path))\n",
    "                last_saved_frame = frame_path\n",
    "                \n",
    "            frame_count += 1\n",
    "            \n",
    "        cap.release()\n",
    "        return frame_pairs\n",
    "\n",
    "class StitchingDataset(Dataset):\n",
    "    \"\"\"Dataset for training image stitching model\"\"\"\n",
    "    def __init__(self, frame_pairs, img_size=224, is_train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            frame_pairs (list): List of (frame1_path, frame2_path) tuples\n",
    "            img_size (int): Size of input images\n",
    "            is_train (bool): Whether to use training augmentations\n",
    "        \"\"\"\n",
    "        self.frame_pairs = frame_pairs\n",
    "        self.img_size = img_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Basic augmentations for both train and test\n",
    "        self.basic_transform = A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        # Additional augmentations for training\n",
    "        if is_train:\n",
    "            self.train_transform = A.Compose([\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.HueSaturationValue(p=0.3),\n",
    "                A.GaussNoise(p=0.2),\n",
    "                A.RandomRotate90(p=0.2),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.frame_pairs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path = self.frame_pairs[idx]\n",
    "        \n",
    "        # Read images\n",
    "        img1 = cv2.imread(str(img1_path))\n",
    "        img2 = cv2.imread(str(img2_path))\n",
    "        \n",
    "        # Convert BGR to RGB\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply training augmentations\n",
    "        if self.is_train:\n",
    "            seed = random.randint(0, 2**32)\n",
    "            \n",
    "            # Apply same random transforms to both images\n",
    "            random.seed(seed)\n",
    "            img1 = self.train_transform(image=img1)[\"image\"]\n",
    "            random.seed(seed)\n",
    "            img2 = self.train_transform(image=img2)[\"image\"]\n",
    "        \n",
    "        # Apply basic transforms\n",
    "        img1 = self.basic_transform(image=img1)[\"image\"]\n",
    "        img2 = self.basic_transform(image=img2)[\"image\"]\n",
    "        \n",
    "        return {\n",
    "            'image1': img1,\n",
    "            'image2': img2,\n",
    "            'path1': str(img1_path),\n",
    "            'path2': str(img2_path)\n",
    "        }\n",
    "\n",
    "def create_dataloaders(video_path, output_dir, batch_size=8, img_size=224, \n",
    "                      num_workers=4, frame_interval=5):\n",
    "    \"\"\"\n",
    "    Create training and validation dataloaders from video\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to input video\n",
    "        output_dir (str): Directory to save extracted frames\n",
    "        batch_size (int): Batch size for dataloaders\n",
    "        img_size (int): Size of input images\n",
    "        num_workers (int): Number of workers for dataloaders\n",
    "        frame_interval (int): Number of frames to skip during extraction\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (train_loader, val_loader)\n",
    "    \"\"\"\n",
    "    # Extract frames\n",
    "    extractor = VideoFrameExtractor(frame_interval=frame_interval)\n",
    "    frame_pairs = extractor.extract_frames(video_path, output_dir)\n",
    "    \n",
    "    # Split into train/val\n",
    "    train_pairs = frame_pairs[:-len(frame_pairs)//5]  # Last 20% for validation\n",
    "    val_pairs = frame_pairs[-len(frame_pairs)//5:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = StitchingDataset(train_pairs, img_size=img_size, is_train=True)\n",
    "    val_dataset = StitchingDataset(val_pairs, img_size=img_size, is_train=False)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "# Utility functions for visualization\n",
    "def visualize_pair(img1, img2, matched_points=None):\n",
    "    \"\"\"\n",
    "    Visualize image pair with optional matched points\n",
    "    \n",
    "    Args:\n",
    "        img1, img2 (torch.Tensor): Input image tensors\n",
    "        matched_points (numpy.ndarray): Matched point pairs\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Visualization image\n",
    "    \"\"\"\n",
    "    # Convert tensors to numpy arrays\n",
    "    img1 = (img1.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    img2 = (img2.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
    "    \n",
    "    # Create side-by-side visualization\n",
    "    h, w = img1.shape[:2]\n",
    "    vis_img = np.zeros((h, w*2, 3), dtype=np.uint8)\n",
    "    vis_img[:, :w] = img1\n",
    "    vis_img[:, w:] = img2\n",
    "    \n",
    "    # Draw matches if provided\n",
    "    if matched_points is not None:\n",
    "        for pt1, pt2 in matched_points:\n",
    "            pt2 = (pt2[0] + w, pt2[1])  # Adjust x-coordinate for second image\n",
    "            cv2.line(vis_img, tuple(map(int, pt1)), tuple(map(int, pt2)), \n",
    "                    (0, 255, 0), 1)\n",
    "            cv2.circle(vis_img, tuple(map(int, pt1)), 3, (255, 0, 0), -1)\n",
    "            cv2.circle(vis_img, tuple(map(int, pt2)), 3, (255, 0, 0), -1)\n",
    "            \n",
    "    return vis_img\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataloaders from video\n",
    "    video_path = \"input_video.mp4\"\n",
    "    output_dir = \"extracted_frames\"\n",
    "    \n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        video_path=video_path,\n",
    "        output_dir=output_dir,\n",
    "        batch_size=8,\n",
    "        img_size=224\n",
    "    )\n",
    "    \n",
    "    # Test dataloader\n",
    "    for batch in train_loader:\n",
    "        img1 = batch['image1'][0]  # Get first image from batch\n",
    "        img2 = batch['image2'][0]\n",
    "        \n",
    "        # Visualize pair\n",
    "        vis_img = visualize_pair(img1, img2)\n",
    "        cv2.imshow('Image Pair', vis_img)\n",
    "        if cv2.waitKey(0) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import wandb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import logging\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        val_loader: torch.utils.data.DataLoader,\n",
    "        config: Dict,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: Model to train\n",
    "            train_loader: Training data loader\n",
    "            val_loader: Validation data loader\n",
    "            config: Training configuration\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # Setup device\n",
    "        self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # Setup optimizer\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config.get('weight_decay', 0.01)\n",
    "        )\n",
    "        \n",
    "        # Setup scheduler\n",
    "        self.scheduler = CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=config['epochs'],\n",
    "            eta_min=config.get('min_lr', 1e-6)\n",
    "        )\n",
    "        \n",
    "        # Setup mixed precision training\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        # Setup logging\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # Setup wandb\n",
    "        if config.get('use_wandb', False):\n",
    "            wandb.init(\n",
    "                project=config.get('wandb_project', 'image-stitching'),\n",
    "                name=config.get('wandb_run_name', time.strftime('%Y%m%d_%H%M%S')),\n",
    "                config=config\n",
    "            )\n",
    "    \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        log_dir = Path(self.config.get('log_dir', 'logs'))\n",
    "        log_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(log_dir / f'training_{time.strftime(\"%Y%m%d_%H%M%S\")}.log'),\n",
    "                logging.StreamHandler()\n",
    "            ]\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        epoch: int,\n",
    "        model_state: Dict,\n",
    "        optimizer_state: Dict,\n",
    "        scheduler_state: Dict,\n",
    "        best_metric: float,\n",
    "        is_best: bool\n",
    "    ):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "        checkpoint_dir = Path(self.config.get('checkpoint_dir', 'checkpoints'))\n",
    "        checkpoint_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state': model_state,\n",
    "            'optimizer_state': optimizer_state,\n",
    "            'scheduler_state': scheduler_state,\n",
    "            'best_metric': best_metric,\n",
    "            'config': self.config\n",
    "        }\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        torch.save(\n",
    "            checkpoint,\n",
    "            checkpoint_dir / 'latest_checkpoint.pth'\n",
    "        )\n",
    "        \n",
    "        # Save best checkpoint\n",
    "        if is_best:\n",
    "            torch.save(\n",
    "                checkpoint,\n",
    "                checkpoint_dir / 'best_checkpoint.pth'\n",
    "            )\n",
    "    \n",
    "    def train_epoch(self) -> Tuple[float, float]:\n",
    "        \"\"\"Train one epoch\"\"\"\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch in pbar:\n",
    "            # Move data to device\n",
    "            img1 = batch['image1'].to(self.device)\n",
    "            img2 = batch['image2'].to(self.device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with autocast():\n",
    "                outputs = self.model(img1, img2)\n",
    "                loss = self.compute_loss(outputs)\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            # Compute metrics\n",
    "            accuracy = self.compute_accuracy(outputs)\n",
    "            \n",
    "            # Update progress bar\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy\n",
    "            pbar.set_postfix({\n",
    "                'loss': total_loss / (pbar.n + 1),\n",
    "                'acc': total_accuracy / (pbar.n + 1)\n",
    "            })\n",
    "        \n",
    "        return total_loss / len(self.train_loader), total_accuracy / len(self.train_loader)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self) -> Tuple[float, float]:\n",
    "        \"\"\"Validate model\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        \n",
    "        pbar = tqdm(self.val_loader, desc='Validation')\n",
    "        for batch in pbar:\n",
    "            # Move data to device\n",
    "            img1 = batch['image1'].to(self.device)\n",
    "            img2 = batch['image2'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = self.model(img1, img2)\n",
    "            loss = self.compute_loss(outputs)\n",
    "            \n",
    "            # Compute metrics\n",
    "            accuracy = self.compute_accuracy(outputs)\n",
    "            \n",
    "            # Update metrics\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += accuracy\n",
    "            pbar.set_postfix({\n",
    "                'loss': total_loss / (pbar.n + 1),\n",
    "                'acc': total_accuracy / (pbar.n + 1)\n",
    "            })\n",
    "        \n",
    "        return total_loss / len(self.val_loader), total_accuracy / len(self.val_loader)\n",
    "    \n",
    "    def compute_loss(self, outputs: Dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute training loss\n",
    "        可以根据需要自定义损失函数\n",
    "        \"\"\"\n",
    "        # Example loss calculation\n",
    "        matching_scores = outputs['matching_scores']\n",
    "        attention_weights = outputs['attention_weights']\n",
    "        \n",
    "        # Matching loss\n",
    "        matching_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            matching_scores,\n",
    "            torch.ones_like(matching_scores)  # Assume all pairs should match\n",
    "        )\n",
    "        \n",
    "        # Attention regularization\n",
    "        attention_loss = -torch.mean(\n",
    "            torch.sum(attention_weights * torch.log(attention_weights + 1e-10), dim=-1)\n",
    "        )\n",
    "        \n",
    "        return matching_loss + 0.1 * attention_loss\n",
    "    \n",
    "    def compute_accuracy(self, outputs: Dict) -> float:\n",
    "        \"\"\"\n",
    "        Compute accuracy metric\n",
    "        可以根据需要自定义准确率计算方法\n",
    "        \"\"\"\n",
    "        matching_scores = outputs['matching_scores']\n",
    "        predictions = (torch.sigmoid(matching_scores) > 0.5).float()\n",
    "        return predictions.mean().item()\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Training loop\"\"\"\n",
    "        best_metric = float('inf')\n",
    "        \n",
    "        for epoch in range(self.config['epochs']):\n",
    "            self.logger.info(f\"Epoch {epoch+1}/{self.config['epochs']}\")\n",
    "            \n",
    "            # Training\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            self.logger.info(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_acc = self.validate()\n",
    "            self.logger.info(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "            \n",
    "            # Update learning rate\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            # Log metrics\n",
    "            if self.config.get('use_wandb', False):\n",
    "                wandb.log({\n",
    "                    'epoch': epoch + 1,\n",
    "                    'train_loss': train_loss,\n",
    "                    'train_acc': train_acc,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_acc': val_acc,\n",
    "                    'learning_rate': self.scheduler.get_last_lr()[0]\n",
    "                })\n",
    "            \n",
    "            # Save checkpoint\n",
    "            is_best = val_loss < best_metric\n",
    "            if is_best:\n",
    "                best_metric = val_loss\n",
    "            \n",
    "            self.save_checkpoint(\n",
    "                epoch=epoch,\n",
    "                model_state=self.model.state_dict(),\n",
    "                optimizer_state=self.optimizer.state_dict(),\n",
    "                scheduler_state=self.scheduler.state_dict(),\n",
    "                best_metric=best_metric,\n",
    "                is_best=is_best\n",
    "            )\n",
    "\n",
    "# Training configuration\n",
    "def get_default_config():\n",
    "    \"\"\"Get default training configuration\"\"\"\n",
    "    return {\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'epochs': 100,\n",
    "        'learning_rate': 1e-4,\n",
    "        'weight_decay': 0.01,\n",
    "        'min_lr': 1e-6,\n",
    "        'use_wandb': False,\n",
    "        'wandb_project': 'image-stitching',\n",
    "        'checkpoint_dir': 'checkpoints',\n",
    "        'log_dir': 'logs'\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Get configuration\n",
    "    config = get_default_config()\n",
    "    \n",
    "    # Create model and data loaders\n",
    "    model = ImageStitchingTransformer()  # Your model\n",
    "    train_loader, val_loader = create_dataloaders(...)  # Your data loaders\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    # Start training\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from typing import Dict, List, Tuple\n",
    "import kornia\n",
    "\n",
    "class StitchingEvaluator:\n",
    "    \"\"\"评估图像拼接模型的性能\"\"\"\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "\n",
    "    def compute_metrics(self, outputs: Dict, batch: Dict) -> Dict[str, float]:\n",
    "        \"\"\"计算多个评估指标\"\"\"\n",
    "        metrics = {}\n",
    "        \n",
    "        # 1. 匹配准确率指标\n",
    "        matching_metrics = self._compute_matching_metrics(\n",
    "            outputs['matching_scores'],\n",
    "            batch['gt_matches']\n",
    "        )\n",
    "        metrics.update(matching_metrics)\n",
    "        \n",
    "        # 2. 几何一致性指标\n",
    "        geometric_metrics = self._compute_geometric_consistency(\n",
    "            outputs['matched_features'],\n",
    "            outputs['attention_weights']\n",
    "        )\n",
    "        metrics.update(geometric_metrics)\n",
    "        \n",
    "        # 3. 重叠区域质量指标\n",
    "        if 'warped_image' in outputs:\n",
    "            quality_metrics = self._compute_image_quality_metrics(\n",
    "                outputs['warped_image'],\n",
    "                batch['image2']\n",
    "            )\n",
    "            metrics.update(quality_metrics)\n",
    "            \n",
    "        return metrics\n",
    "    \n",
    "    def _compute_matching_metrics(self, pred_scores: torch.Tensor, \n",
    "                                gt_matches: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"计算特征匹配的准确率指标\"\"\"\n",
    "        pred_matches = (torch.sigmoid(pred_scores) > 0.5).cpu().numpy()\n",
    "        gt_matches = gt_matches.cpu().numpy()\n",
    "        \n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            gt_matches.ravel(),\n",
    "            pred_matches.ravel(),\n",
    "            average='binary'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'matching_precision': precision,\n",
    "            'matching_recall': recall,\n",
    "            'matching_f1': f1\n",
    "        }\n",
    "    \n",
    "    def _compute_geometric_consistency(self, matched_features: torch.Tensor,\n",
    "                                    attention_weights: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"计算几何一致性指标\"\"\"\n",
    "        # 1. 局部一致性评分\n",
    "        local_consistency = self._compute_local_consistency(matched_features)\n",
    "        \n",
    "        # 2. 全局变换评分\n",
    "        global_consistency = self._compute_global_consistency(matched_features)\n",
    "        \n",
    "        return {\n",
    "            'local_geometric_consistency': local_consistency,\n",
    "            'global_geometric_consistency': global_consistency\n",
    "        }\n",
    "    \n",
    "    def _compute_image_quality_metrics(self, warped_image: torch.Tensor,\n",
    "                                     target_image: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"计算图像质量指标\"\"\"\n",
    "        # 计算PSNR\n",
    "        psnr = kornia.metrics.psnr(warped_image, target_image, max_val=1.0)\n",
    "        \n",
    "        # 计算SSIM\n",
    "        ssim = kornia.metrics.ssim(warped_image, target_image, window_size=11)\n",
    "        \n",
    "        return {\n",
    "            'psnr': psnr.item(),\n",
    "            'ssim': ssim.mean().item()\n",
    "        }\n",
    "\n",
    "class ImageStitcher:\n",
    "    \"\"\"图像拼接推理类\"\"\"\n",
    "    def __init__(self, model: torch.nn.Module, config: Dict):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def preprocess_image(self, image: np.ndarray) -> torch.Tensor:\n",
    "        \"\"\"预处理输入图像\"\"\"\n",
    "        # 调整图像大小\n",
    "        image = cv2.resize(image, (self.config['img_size'], self.config['img_size']))\n",
    "        \n",
    "        # 转换为RGB\n",
    "        if len(image.shape) == 2:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 4:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGRA2RGB)\n",
    "        elif image.shape[2] == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        # 归一化和转换为tensor\n",
    "        image = to_tensor(image)\n",
    "        image = F.normalize(image, \n",
    "                          mean=[0.485, 0.456, 0.406],\n",
    "                          std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def stitch_images(self, img1: np.ndarray, img2: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"拼接两张图像\"\"\"\n",
    "        # 预处理\n",
    "        tensor1 = self.preprocess_image(img1).unsqueeze(0).to(self.device)\n",
    "        tensor2 = self.preprocess_image(img2).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        # 模型推理\n",
    "        outputs = self.model(tensor1, tensor2)\n",
    "        \n",
    "        # 获取匹配点\n",
    "        matched_points = self._get_matched_points(outputs)\n",
    "        \n",
    "        # 估计变换矩阵\n",
    "        H = self._estimate_transform(matched_points)\n",
    "        \n",
    "        # 图像融合\n",
    "        stitched_image = self._blend_images(img1, img2, H)\n",
    "        \n",
    "        return stitched_image, {\n",
    "            'matched_points': matched_points,\n",
    "            'homography': H,\n",
    "            'confidence_scores': outputs['matching_scores'].cpu().numpy()\n",
    "        }\n",
    "    \n",
    "    def _get_matched_points(self, outputs: Dict) -> np.ndarray:\n",
    "        \"\"\"从模型输出中获取匹配点对\"\"\"\n",
    "        scores = torch.sigmoid(outputs['matching_scores'])\n",
    "        matches = scores > self.config.get('matching_threshold', 0.5)\n",
    "        \n",
    "        # 转换为图像坐标\n",
    "        matched_indices = torch.nonzero(matches).cpu().numpy()\n",
    "        matched_points = []\n",
    "        \n",
    "        for idx1, idx2 in matched_indices:\n",
    "            pt1 = self._index_to_coordinate(idx1)\n",
    "            pt2 = self._index_to_coordinate(idx2)\n",
    "            matched_points.append((pt1, pt2))\n",
    "            \n",
    "        return np.array(matched_points)\n",
    "    \n",
    "    def _estimate_transform(self, matched_points: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"估计变换矩阵\"\"\"\n",
    "        if len(matched_points) < 4:\n",
    "            raise ValueError(\"Not enough matched points for homography estimation\")\n",
    "            \n",
    "        H, mask = cv2.findHomography(\n",
    "            matched_points[:, 0],\n",
    "            matched_points[:, 1],\n",
    "            cv2.RANSAC,\n",
    "            5.0\n",
    "        )\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    def _blend_images(self, img1: np.ndarray, img2: np.ndarray, \n",
    "                     H: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"融合图像\"\"\"\n",
    "        # 计算变换后图像的大小\n",
    "        h1, w1 = img1.shape[:2]\n",
    "        h2, w2 = img2.shape[:2]\n",
    "        corners1 = np.float32([[0, 0], [0, h1], [w1, h1], [w1, 0]]).reshape(-1, 1, 2)\n",
    "        corners2 = np.float32([[0, 0], [0, h2], [w2, h2], [w2, 0]]).reshape(-1, 1, 2)\n",
    "        corners2_warped = cv2.perspectiveTransform(corners2, H)\n",
    "        corners = np.concatenate([corners1, corners2_warped], axis=0)\n",
    "        \n",
    "        [xmin, ymin] = np.int32(corners.min(axis=0).ravel() - 0.5)\n",
    "        [xmax, ymax] = np.int32(corners.max(axis=0).ravel() + 0.5)\n",
    "        t = [-xmin, -ymin]\n",
    "        \n",
    "        # 创建输出图像\n",
    "        Ht = np.array([[1, 0, t[0]], [0, 1, t[1]], [0, 0, 1]])\n",
    "        img1_warped = cv2.warpPerspective(img1, Ht, (xmax-xmin, ymax-ymin))\n",
    "        img2_warped = cv2.warpPerspective(img2, Ht.dot(H), (xmax-xmin, ymax-ymin))\n",
    "        \n",
    "        # 简单的加权融合\n",
    "        mask1 = (img1_warped != 0).all(axis=2)\n",
    "        mask2 = (img2_warped != 0).all(axis=2)\n",
    "        overlap = mask1 & mask2\n",
    "        \n",
    "        # 在重叠区域进行加权融合\n",
    "        result = img1_warped.copy()\n",
    "        result[overlap] = (img1_warped[overlap] * 0.5 + img2_warped[overlap] * 0.5).astype(np.uint8)\n",
    "        result[~mask1 & mask2] = img2_warped[~mask1 & mask2]\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load model\n",
    "    model = ImageStitchingTransformer()  # Your model\n",
    "    checkpoint = torch.load('best_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    \n",
    "    # Create stitcher\n",
    "    config = {\n",
    "        'device': 'cuda',\n",
    "        'img_size': 224,\n",
    "        'matching_threshold': 0.5\n",
    "    }\n",
    "    stitcher = ImageStitcher(model, config)\n",
    "    \n",
    "    # Read images\n",
    "    img1 = cv2.imread('image1.jpg')\n",
    "    img2 = cv2.imread('image2.jpg')\n",
    "    \n",
    "    # Stitch images\n",
    "    result, info = stitcher.stitch_images(img1, img2)\n",
    "    \n",
    "    # Save result\n",
    "    cv2.imwrite('stitched_image.jpg', result)\n",
    "    \n",
    "    # Create evaluator and compute metrics\n",
    "    evaluator = StitchingEvaluator()\n",
    "    metrics = evaluator.compute_metrics(\n",
    "        outputs={'matching_scores': torch.tensor([...])},  # Model outputs\n",
    "        batch={'gt_matches': torch.tensor([...])}  # Ground truth\n",
    "    )\n",
    "    print(\"Evaluation metrics:\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
