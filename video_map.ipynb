{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import kornia\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一部分：模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    图像patch嵌入模块\n",
    "    将输入图像划分为固定大小的patch，并进行特征提取\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # 使用卷积层进行patch划分和特征提取\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c'),\n",
    "        )\n",
    "        \n",
    "        # 可学习的位置编码\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        # 添加分类token和位置编码\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b=B)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        return x\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    多头自注意力机制\n",
    "    用于捕捉图像patch之间的长程依赖关系\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # 注意力加权求和\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer编码器块\n",
    "    包含多头自注意力和前馈神经网络\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = MultiHeadAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, \n",
    "                                     attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(mlp_hidden_dim, dim),\n",
    "            nn.Dropout(drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 残差连接\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViTFeatureExtractor(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer特征提取器\n",
    "    用于提取图像的层级特征表示\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(embed_dim, num_heads, mlp_ratio, qkv_bias, drop_rate, attn_drop_rate)\n",
    "            for _ in range(depth)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class CrossAttentionMatcher(nn.Module):\n",
    "    \"\"\"\n",
    "    交叉注意力匹配模块\n",
    "    用于计算两张图像特征之间的相似度\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, num_heads=8, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(dim, dim)\n",
    "        self.k_proj = nn.Linear(dim, dim)\n",
    "        self.v_proj = nn.Linear(dim, dim)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        B, N, C = x1.shape\n",
    "        # 计算查询、键和值\n",
    "        q = self.q_proj(x1).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        k = self.k_proj(x2).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        v = self.v_proj(x2).reshape(B, N, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "\n",
    "        # 计算注意力分数\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # 注意力加权求和\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        \n",
    "        return x, attn\n",
    "\n",
    "class ImageStitchingTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    完整的图像拼接Transformer模型\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, drop_rate=0., attn_drop_rate=0.):\n",
    "        super().__init__()\n",
    "        # 特征提取器\n",
    "        self.feature_extractor = ViTFeatureExtractor(\n",
    "            img_size, patch_size, in_channels, embed_dim, depth, \n",
    "            num_heads, mlp_ratio, qkv_bias, drop_rate, attn_drop_rate\n",
    "        )\n",
    "        # 特征匹配器\n",
    "        self.matcher = CrossAttentionMatcher(embed_dim, num_heads, attn_drop_rate, drop_rate)\n",
    "        \n",
    "        # 匹配得分头\n",
    "        self.match_head = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        # 提取特征\n",
    "        feat1 = self.feature_extractor(img1)\n",
    "        feat2 = self.feature_extractor(img2)\n",
    "        \n",
    "        # 交叉注意力匹配\n",
    "        matched_features, attention_weights = self.matcher(feat1, feat2)\n",
    "        \n",
    "        # 生成匹配分数\n",
    "        matching_scores = self.match_head(matched_features).squeeze(-1)\n",
    "        \n",
    "        return {\n",
    "            'features1': feat1,\n",
    "            'features2': feat2,\n",
    "            'matched_features': matched_features,\n",
    "            'attention_weights': attention_weights,\n",
    "            'matching_scores': matching_scores\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二部分：数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoFrameExtractor:\n",
    "    \"\"\"\n",
    "    视频帧提取器\n",
    "    从输入视频中提取帧用于训练\n",
    "    \"\"\"\n",
    "    def __init__(self, overlap_ratio=0.3, frame_interval=5):\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.frame_interval = frame_interval\n",
    "        \n",
    "    def extract_frames(self, video_path, output_dir):\n",
    "        \"\"\"提取视频帧并保存\"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            raise ValueError(f\"无法打开视频文件: {video_path}\")\n",
    "            \n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        frame_pairs = []\n",
    "        frame_count = 0\n",
    "        last_saved_frame = None\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            if frame_count % self.frame_interval == 0:\n",
    "                frame_path = output_dir / f\"frame_{frame_count:06d}.jpg\"\n",
    "                cv2.imwrite(str(frame_path), frame)\n",
    "                \n",
    "                if last_saved_frame is not None:\n",
    "                    frame_pairs.append((last_saved_frame, frame_path))\n",
    "                last_saved_frame = frame_path\n",
    "                \n",
    "            frame_count += 1\n",
    "            \n",
    "        cap.release()\n",
    "        return frame_pairs\n",
    "    class StitchingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    图像拼接数据集\n",
    "    用于训练图像拼接模型\n",
    "    \"\"\"\n",
    "    def __init__(self, frame_pairs, img_size=224, is_train=True):\n",
    "        self.frame_pairs = frame_pairs\n",
    "        self.img_size = img_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # 基础图像变换\n",
    "        self.basic_transform = A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        # 训练时的数据增强\n",
    "        if is_train:\n",
    "            self.train_transform = A.Compose([\n",
    "                A.RandomBrightnessContrast(p=0.5),\n",
    "                A.HueSaturationValue(p=0.3),\n",
    "                A.GaussNoise(p=0.2),\n",
    "                A.RandomRotate90(p=0.2),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.frame_pairs)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img1_path, img2_path = self.frame_pairs[idx]\n",
    "        \n",
    "        # 读取图像\n",
    "        img1 = cv2.imread(str(img1_path))\n",
    "        img2 = cv2.imread(str(img2_path))\n",
    "        \n",
    "        # BGR转RGB\n",
    "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 训练时进行数据增强\n",
    "        if self.is_train:\n",
    "            seed = random.randint(0, 2**32)\n",
    "            \n",
    "            # 对两张图像应用相同的随机变换\n",
    "            random.seed(seed)\n",
    "            img1 = self.train_transform(image=img1)[\"image\"]\n",
    "            random.seed(seed)\n",
    "            img2 = self.train_transform(image=img2)[\"image\"]\n",
    "        \n",
    "        # 应用基础变换\n",
    "        img1 = self.basic_transform(image=img1)[\"image\"]\n",
    "        img2 = self.basic_transform(image=img2)[\"image\"]\n",
    "        \n",
    "        return {\n",
    "            'image1': img1,\n",
    "            'image2': img2,\n",
    "            'path1': str(img1_path),\n",
    "            'path2': str(img2_path)\n",
    "        }\n",
    "\n",
    "def create_dataloaders(video_path, output_dir, batch_size=8, img_size=224, \n",
    "                      num_workers=4, frame_interval=5):\n",
    "    \"\"\"\n",
    "    创建训练和验证数据加载器\n",
    "    \n",
    "    参数:\n",
    "        video_path: 输入视频路径\n",
    "        output_dir: 帧保存目录\n",
    "        batch_size: 批次大小\n",
    "        img_size: 图像大小\n",
    "        num_workers: 数据加载进程数\n",
    "        frame_interval: 帧采样间隔\n",
    "    \"\"\"\n",
    "    # 提取视频帧\n",
    "    extractor = VideoFrameExtractor(frame_interval=frame_interval)\n",
    "    frame_pairs = extractor.extract_frames(video_path, output_dir)\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    train_pairs = frame_pairs[:-len(frame_pairs)//5]  # 后20%用于验证\n",
    "    val_pairs = frame_pairs[-len(frame_pairs)//5:]\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = StitchingDataset(train_pairs, img_size=img_size, is_train=True)\n",
    "    val_dataset = StitchingDataset(val_pairs, img_size=img_size, is_train=False)\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    模型训练器\n",
    "    管理整个训练过程，包括日志记录、检查点保存等\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        val_loader: torch.utils.data.DataLoader,\n",
    "        config: Dict,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.config = config\n",
    "        \n",
    "        # 设置设备\n",
    "        self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "        self.model = self.model.to(self.device)\n",
    "        \n",
    "        # 设置优化器\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config.get('weight_decay', 0.01)\n",
    "        )\n",
    "        \n",
    "        # 设置学习率调度器\n",
    "        self.scheduler = CosineAnnealingLR(\n",
    "            self.optimizer,\n",
    "            T_max=config['epochs'],\n",
    "            eta_min=config.get('min_lr', 1e-6)\n",
    "        )\n",
    "        \n",
    "        # 设置混合精度训练\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "        # 设置日志\n",
    "        self.setup_logging()\n",
    "        \n",
    "        # 设置wandb\n",
    "        if config.get('use_wandb', False):\n",
    "            wandb.init(\n",
    "                project=config.get('wandb_project', 'image-stitching'),\n",
    "                name=config.get('wandb_run_name', time.strftime('%Y%m%d_%H%M%S')),\n",
    "                config=config\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
